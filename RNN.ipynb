{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokKaturi/Advance-Applications-of-AI-/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zERB9w6gYIg-"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "RNNs capture patterns in sequences of data. In this case, the sequence will be characters in the names of chemical compounds, which are often composed of parts like prefixes (e.g. \"mono-\", \"methyl-\"), roots (\"fluor\", \"nitr\"), and suffixes (e.g. \"-ate\", \"-ide\"). We'll see if a simple RNN can pick up on these patterns and generate new chemical compounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn-rEyNLYIg_"
      },
      "source": [
        "## Objective\n",
        "\n",
        "This will be a multi-input, multi-output RNN. We will train it to predict the next character at each step. This makes it a 'language model', and also a type of autoencoder!\n",
        "\n",
        "During training, the correct label is simply the following character, which means the labels will be obtained by simply offsetting the full string one to the left (see figure below). During inference, the best guess output for a given time step will be passed to the next step as input.\n",
        "\n",
        "In order to generate variable length sequences, we'll add characters to signify the start ('<') and end ('>') of a compound name. At inference time, we will pass '<' to the network to start the sequence, and stop generation when the network outputs '>'.\n",
        "\n",
        "### Training\n",
        "\n",
        "![train](https://drive.google.com/uc?id=1_dn3LMkMnWz8CMXT7xayGDdq5S9aVY3V)\n",
        "\n",
        "### Inference\n",
        "\n",
        "![inference](https://drive.google.com/uc?id=1kMKxCukw9M5K6CA2UxO-eQBxLqAa4O0B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "82u1EVg9YIg_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "KS5PcULdYIhA"
      },
      "outputs": [],
      "source": [
        "text = open('compounds.txt', 'rb').read()\n",
        "text = text.decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeFzkR7FYIhA",
        "outputId": "dde9548c-97ac-4ee4-fefc-5036da836a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab:\n",
            " ['\\n', ' ', \"'\", '(', ')', ',', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '>', '<']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the corpus\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# We'll use < and > to denote start and end\n",
        "vocab.append('>')\n",
        "vocab.append('<')\n",
        "\n",
        "print('Vocab:\\n', vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QzKKzc-YIhA"
      },
      "source": [
        "Neural networks operate on vectors of continuous values, not lists of discrete values like characters. We will need to vectorize the input using \"one-hot\" encoding, in which the vector is the size of the \"vocabulary\" (in this case all characters we saw in the input). For each input character, the one-hot vector will have a 1 in the dimension representing that character and 0s everywhere else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4cUDnJJYIhA",
        "outputId": "400276db-fd00-414a-ce1e-c9c647903cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 67\n"
          ]
        }
      ],
      "source": [
        "# Create a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "# Make a copy of the unique set elements in NumPy array format for later use in the decoding the predictions\n",
        "idx2char = np.array(vocab)\n",
        "# Vectorize the text with a for loop\n",
        "lines = text.split('\\n')\n",
        "lines = list(map(lambda x: '<' + x + '>', lines))\n",
        "max_len = np.max(list(map(len, lines)))\n",
        "print(\"Max length:\", max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seEnXbvrYIhB"
      },
      "source": [
        "In order to generate variable length sequences, we'll add characters to signify the start ('<') and end ('>') of a compound name. At inference time, we will pass '<' to the network to start the sequence, and stop generation when the network outputs '>'.\n",
        "\n",
        "We will also pad the training names with zeros so they are all the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "iHVHwC0wYIhB"
      },
      "outputs": [],
      "source": [
        "source=[]\n",
        "target=[]\n",
        "for line in lines:\n",
        "    padded=np.zeros(max_len)\n",
        "    for i, c in enumerate(line):\n",
        "        padded[i] = char2idx[c]\n",
        "    source.append(padded[:-1])\n",
        "    target.append(padded[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLp8poWnYIhB"
      },
      "source": [
        "Let's examine an encoded training example. Notice how the target is actually just the source, but shifted by one, because our network is being tasked with looking one time step into the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5PbtYYzYIhB",
        "outputId": "5332db73-c357-4b08-c45b-ee8cb7ee5a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            "<(1r,3r)-1,2,3-trimethylcyclopentane>\n",
            "\n",
            "Vector encoded source:\n",
            "[47.  3.  9. 37.  5. 11. 37.  4.  6.  9.  5. 10.  5. 11.  6. 39. 37. 28.\n",
            " 32. 24. 39. 27. 44. 31. 22. 44. 22. 31. 34. 35. 24. 33. 39. 20. 33. 24.\n",
            " 46.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            "\n",
            "Vector encoded target:\n",
            "[ 3.  9. 37.  5. 11. 37.  4.  6.  9.  5. 10.  5. 11.  6. 39. 37. 28. 32.\n",
            " 24. 39. 27. 44. 31. 22. 44. 22. 31. 34. 35. 24. 33. 39. 20. 33. 24. 46.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
          ]
        }
      ],
      "source": [
        "print(\"Text:\")\n",
        "print(lines[0])\n",
        "print(\"\\nVector encoded source:\")\n",
        "print(source[0])\n",
        "print(\"\\nVector encoded target:\")\n",
        "print(target[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "59j0dyLuYIhB"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((source, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "PvUa4_2sYIhB"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000 # TF shuffles the data only within buffers\n",
        "BATCH_SIZE = 1 # Batch size\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rROhWcgYIhC"
      },
      "source": [
        "**1. Add a [SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) layer with 128 units. Make the layer return sequences, since this RNN will have an output at each time step.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "4Yhc7xJ2YIhC"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[BATCH_SIZE, None]),\n",
        "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDi04tSKYIhC",
        "outputId": "6bc776ce-6aea-4415-adf5-84f460ffc11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_17 (Embedding)    (1, None, 64)             3072      \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (1, None, 128)            24704     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (1, None, 48)             6192      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,968\n",
            "Trainable params: 33,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zAVePDY0YIhC"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zofDJ3MCYIhC"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, max_preds):\n",
        "    input_list = [char2idx['<']] # start with the start char; the rest is zeros\n",
        "    text_generated = [] # Empty string to store our results\n",
        "    model.reset_states() # Clears the hidden states in the RNN\n",
        "\n",
        "    for i in range(max_preds-1): #Run a loop for number of characters to generate\n",
        "        predictions = model(tf.expand_dims(input_list, 0)) # prediction for single character\n",
        "        predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # The predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        # So the model makes the next prediction based on the previous character\n",
        "        input_list.append(predicted_id)\n",
        "        # Also devectorize the number and add to the generated text\n",
        "        if predicted_id==0:\n",
        "            continue\n",
        "        pred_char=idx2char[predicted_id]\n",
        "        if pred_char=='>':\n",
        "            break\n",
        "        text_generated.append(pred_char)\n",
        "\n",
        "    return ''.join(text_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tHzYmXRYIhC"
      },
      "source": [
        "Train the network for 5 epochs (this will take a few minutes). We will generate a few examples each epoch to check progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOVx6XsoYIhC",
        "outputId": "f57e18ae-69b1-4c96-eba8-c1b3dc9f0dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1\n",
            "3542/3542 [==============================] - 198s 55ms/step - loss: 0.5585\n",
            "EXAMPLES:\n",
            "tramidium\n",
            "dethyl9um(iii) oxydith\n",
            "15-99-5\n",
            "ium ctil\n",
            "atilive pengoperoxe\n",
            "\n",
            "EPOCH 2\n",
            "3542/3542 [==============================] - 189s 53ms/step - loss: 0.3603\n",
            "EXAMPLES:\n",
            "chlorum(iii) phosphide\n",
            "perchlorate\n",
            "dichloroiuon tetra2e\n",
            "calcin\n",
            "germoly solicon\n",
            "\n",
            "EPOCH 3\n",
            "3542/3542 [==============================] - 192s 54ms/step - loss: 0.3089\n",
            "EXAMPLES:\n",
            "magnesium olane\n",
            "uranium(iii) telluride\n",
            "carconiim\n",
            "calpium oxale\n",
            "alicalamogen arbonol\n",
            "\n",
            "EPOCH 4\n",
            "3542/3542 [==============================] - 190s 54ms/step - loss: 0.2845\n",
            "EXAMPLES:\n",
            "goly dinium(iii) chloride\n",
            "zirconium difluoride\n",
            "argentum texane\n",
            "caesium chromite\n",
            "sulfur dioxide\n",
            "\n",
            "EPOCH 5\n",
            "3542/3542 [==============================] - 188s 53ms/step - loss: 0.2712\n",
            "EXAMPLES:\n",
            "sodium brseurite\n",
            "erbiomonyb-alubrol\n",
            "chrosulvarsenium(iii) chloride\n",
            "manganesen(v) oxide\n",
            "copper(ii) bromide\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs=5\n",
        "for i in range(epochs):\n",
        "    print(\"EPOCH %d\"%(i+1))\n",
        "    model.fit(dataset, epochs=1, batch_size=BATCH_SIZE)\n",
        "    print(\"EXAMPLES:\")\n",
        "    for i in range(5):\n",
        "        generated_text = generate_text(model, max_preds=max_len)\n",
        "        print(generated_text)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDmytOJ-YIhD"
      },
      "source": [
        "Try generating some more examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVn2Ro7KYIhD",
        "outputId": "453240d1-c5fd-4ad3-8906-4c2ee06270ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liburium(iv) oxide\n",
            "mangane1\n",
            "dichloropanate\n",
            "borate ioc\n",
            "thorium dichromide\n",
            "caesium phosphate\n",
            "strontinol\n",
            "indium(iii) iodide\n",
            "urrchloroiodide\n",
            "silicon tetraiodide\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    generated_text = generate_text(model, max_preds=max_len)\n",
        "    print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVHP7AR8YIhD"
      },
      "source": [
        "Let's try a more advanced network. LSTMs carry an additional hidden state vector over each time step, with learned gates controlling the flow into and out of this carryover vector.\n",
        "\n",
        "We'll also try stacking RNN layers.\n",
        "\n",
        "**2. Stack two [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) layers, each with 64 units.**\n",
        "  - Hint: A recurrent layer that is passing activations to a subsequent recurrent layer always has to return sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "wDwDAfkQYIhD"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[BATCH_SIZE, None]),\n",
        "    tf.keras.layers.LSTM(64,return_sequences=True),                      \n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    \n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6DpL3hme8gm",
        "outputId": "c8c539fe-654c-460f-a86f-297bfbb54121"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_21 (Embedding)    (1, None, 64)             3072      \n",
            "                                                                 \n",
            " lstm_31 (LSTM)              (1, None, 64)             33024     \n",
            "                                                                 \n",
            " lstm_32 (LSTM)              (1, None, 64)             33024     \n",
            "                                                                 \n",
            " dense_21 (Dense)            (1, None, 48)             3120      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72,240\n",
            "Trainable params: 72,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "hjqLz9K9YIhD"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPpfNG7vYIhD"
      },
      "source": [
        "Train the new network for 5 epochs (this will take a few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9NbAKddYIhD",
        "outputId": "1eb4c428-dab5-415c-a28d-38642a2af04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1\n",
            "3542/3542 [==============================] - 63s 8ms/step - loss: 0.6993\n",
            "EXAMPLES:\n",
            "wiumsolmium chobid achluride\n",
            "tatvlolu(ii) solxide\n",
            "glorpet(ii) scifluoride\n",
            "mosseee in\n",
            "cerleavg loride\n",
            "\n",
            "EPOCH 2\n",
            "3542/3542 [==============================] - 27s 8ms/step - loss: 0.4044\n",
            "EXAMPLES:\n",
            "scoroterzone sulfate metroxypibhate\n",
            "zitrlenifon(iii) arms-bicarge\n",
            "tallium(iv) chloride\n",
            "slenicyane acate\n",
            "terylonide\n",
            "\n",
            "EPOCH 3\n",
            "3542/3542 [==============================] - 27s 8ms/step - loss: 0.3195\n",
            "EXAMPLES:\n",
            "usthamonicoancomynase\n",
            "pyrborine clomide\n",
            "ladm(iv) tetrachloride\n",
            "cerium hybide\n",
            "proldoxine\n",
            "\n",
            "EPOCH 4\n",
            "3542/3542 [==============================] - 26s 7ms/step - loss: 0.2847\n",
            "EXAMPLES:\n",
            "disodium hexachlorote\n",
            "terium manganeatetroagium(iii) iodide\n",
            "nickel(ii) carbonade\n",
            "diphosphalum hypochlorite\n",
            "tin(ii) seleuite\n",
            "\n",
            "EPOCH 5\n",
            "3542/3542 [==============================] - 28s 8ms/step - loss: 0.2651\n",
            "EXAMPLES:\n",
            "berfluorcoside hin\n",
            "phosphine\n",
            "indoline\n",
            "cyanomethoxyandic iodide\n",
            "cobalt(ii) sulfide\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs=5\n",
        "for i in range(epochs):\n",
        "    print(\"EPOCH %d\"%(i+1))\n",
        "    model.fit(dataset, epochs=1, batch_size=BATCH_SIZE)\n",
        "    print(\"EXAMPLES:\")\n",
        "    for i in range(5):\n",
        "        generated_text = generate_text(model, max_preds=max_len)\n",
        "        print(generated_text)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxIU-QpNYIhD"
      },
      "source": [
        "Do the LSTM-generated compound names look any different than the simple RNN?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-fFetnXYIhD",
        "outputId": "b78fd99d-4639-4e67-c6a8-011bc7b1b8f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sodium metnathiofluoride\n",
            "sodium iodide\n",
            "potassium metahydrate trioxide\n",
            "surythiene\n",
            "nickel(ii) bromide\n",
            "mangatesine monofluoride\n",
            "t,ngslium hydroxide\n",
            "sodium monoxide\n",
            "lead(ii) chromate\n",
            "aluminium mitosstlohfluoride\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    generated_text = generate_text(model, max_preds=max_len)\n",
        "    print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}